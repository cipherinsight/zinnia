{
  "crypt::ecc": {
    "zinnia": {
      "name": "crypt::ecc.py",
      "proving_time": 1.0425932954,
      "snark_size": 3276,
      "advice_cells": 252,
      "fixed_cells": 4,
      "range_check_advice_cells": 0,
      "verify_time": 0.0039586404
    },
    "halo2": {
      "name": "crypt::ecc.rs",
      "proving_time": 1.0809336418000002,
      "snark_size": 3276,
      "advice_cells": 258,
      "fixed_cells": 4,
      "range_check_advice_cells": 0,
      "verify_time": 0.0035180134
    },
    "zinnia_compile_time": 0.006844449043273926
  },
  "crypt::poseidon": {
    "zinnia": {
      "name": "crypt::poseidon.py",
      "proving_time": 1.1365771683,
      "snark_size": 3308,
      "advice_cells": 15854,
      "fixed_cells": 327,
      "range_check_advice_cells": 0,
      "verify_time": 0.0041473112
    },
    "halo2": {
      "name": "crypt::poseidon.rs",
      "proving_time": 1.1388271758,
      "snark_size": 3308,
      "advice_cells": 15854,
      "fixed_cells": 327,
      "range_check_advice_cells": 0,
      "verify_time": 0.003968885
    },
    "zinnia_compile_time": 0.0046487092971801754
  },
  "mlalgo::neuron": {
    "zinnia": {
      "name": "mlalgo::neuron.py",
      "proving_time": 2.5005591141,
      "snark_size": 10992,
      "advice_cells": 354810,
      "fixed_cells": 39,
      "range_check_advice_cells": 97936,
      "verify_time": 0.0086665784
    },
    "halo2": {
      "name": "mlalgo::neuron.rs",
      "proving_time": 2.4942062417999997,
      "snark_size": 10992,
      "advice_cells": 380900,
      "fixed_cells": 39,
      "range_check_advice_cells": 97890,
      "verify_time": 0.0105432744
    },
    "zinnia_compile_time": 6.017771911621094
  },
  "mlalgo::kmeans": {
    "zinnia": {
      "name": "mlalgo::kmeans.py",
      "proving_time": 2.4314916236000004,
      "snark_size": 10992,
      "advice_cells": 337120,
      "fixed_cells": 45,
      "range_check_advice_cells": 96148,
      "verify_time": 0.0091983137
    },
    "halo2": {
      "name": "mlalgo::kmeans.rs",
      "proving_time": 2.4942324843,
      "snark_size": 10992,
      "advice_cells": 354413,
      "fixed_cells": 45,
      "range_check_advice_cells": 99820,
      "verify_time": 0.0082744638
    },
    "zinnia_compile_time": 4.215997433662414
  },
  "mlalgo::linear_regression": {
    "zinnia": {
      "name": "mlalgo::linear_regression.py",
      "proving_time": 6.2962538286,
      "snark_size": 33324,
      "advice_cells": 1297398,
      "fixed_cells": 46,
      "range_check_advice_cells": 396751,
      "verify_time": 0.0171572993
    },
    "halo2": {
      "name": "mlalgo::linear_regression.rs",
      "proving_time": 9.9350026803,
      "snark_size": 54336,
      "advice_cells": 2256351,
      "fixed_cells": 46,
      "range_check_advice_cells": 690151,
      "verify_time": 0.0218646988
    },
    "zinnia_compile_time": 12.951625776290893
  },
  "leetcode_array::p204": {
    "zinnia": {
      "name": "leetcode_array::p204.py",
      "proving_time": 1.3790682882999998,
      "snark_size": 3844,
      "advice_cells": 34218,
      "fixed_cells": 1023,
      "range_check_advice_cells": 46,
      "verify_time": 0.005480394099999999
    },
    "halo2": {
      "name": "leetcode_array::p204.rs",
      "proving_time": 1.6999211949,
      "snark_size": 5676,
      "advice_cells": 108756,
      "fixed_cells": 1023,
      "range_check_advice_cells": 46,
      "verify_time": 0.008764517500000001
    },
    "zinnia_compile_time": 25.33265891075134
  },
  "leetcode_array::p832": {
    "zinnia": {
      "name": "leetcode_array::p832.py",
      "proving_time": 1.0778732836999998,
      "snark_size": 3276,
      "advice_cells": 8200,
      "fixed_cells": 2,
      "range_check_advice_cells": 0,
      "verify_time": 0.0037320124999999996
    },
    "halo2": {
      "name": "leetcode_array::p832.rs",
      "proving_time": 1.0746954964000002,
      "snark_size": 3276,
      "advice_cells": 8200,
      "fixed_cells": 2,
      "range_check_advice_cells": 0,
      "verify_time": 0.0045618405
    },
    "zinnia_compile_time": 0.7962660789489746
  },
  "leetcode_dp::p740": {
    "zinnia": {
      "name": "leetcode_dp::p740.py",
      "proving_time": 1.3641435306000003,
      "snark_size": 3844,
      "advice_cells": 14865,
      "fixed_cells": 44,
      "range_check_advice_cells": 1380,
      "verify_time": 0.004891948799999999
    },
    "halo2": {
      "name": "leetcode_dp::p740.rs",
      "proving_time": 1.3528787916,
      "snark_size": 3844,
      "advice_cells": 15385,
      "fixed_cells": 44,
      "range_check_advice_cells": 1403,
      "verify_time": 0.0046596811
    },
    "zinnia_compile_time": 0.3619699239730835
  },
  "leetcode_dp::p1137": {
    "zinnia": {
      "name": "leetcode_dp::p1137.py",
      "proving_time": 1.0533153057,
      "snark_size": 3276,
      "advice_cells": 3652,
      "fixed_cells": 192,
      "range_check_advice_cells": 0,
      "verify_time": 0.0042784942999999995
    },
    "halo2": {
      "name": "leetcode_dp::p1137.rs",
      "proving_time": 1.0600317372,
      "snark_size": 3276,
      "advice_cells": 4425,
      "fixed_cells": 101,
      "range_check_advice_cells": 0,
      "verify_time": 0.0040031145
    },
    "zinnia_compile_time": 0.12533364295959473
  },
  "leetcode_graph::p3112": {
    "zinnia": {
      "name": "leetcode_graph::p3112.py",
      "proving_time": 1.3312789309,
      "snark_size": 3844,
      "advice_cells": 1620,
      "fixed_cells": 25,
      "range_check_advice_cells": 230,
      "verify_time": 0.0046196227
    },
    "halo2": {
      "name": "leetcode_graph::p3112.rs",
      "proving_time": 1.7208763283,
      "snark_size": 6384,
      "advice_cells": 139781,
      "fixed_cells": 25,
      "range_check_advice_cells": 23230,
      "verify_time": 0.0065913401
    },
    "zinnia_compile_time": 6.091324877738953
  },
  "leetcode_graph::p997": {
    "zinnia": {
      "name": "leetcode_graph::p997.py",
      "proving_time": 1.0783498015999997,
      "snark_size": 3276,
      "advice_cells": 6585,
      "fixed_cells": 10,
      "range_check_advice_cells": 0,
      "verify_time": 0.0048637452
    },
    "halo2": {
      "name": "leetcode_graph::p997.rs",
      "proving_time": 1.0486571949999999,
      "snark_size": 3276,
      "advice_cells": 10101,
      "fixed_cells": 10,
      "range_check_advice_cells": 0,
      "verify_time": 0.005749450299999999
    },
    "zinnia_compile_time": 0.40155725479125975
  },
  "leetcode_math::p492": {
    "zinnia": {
      "name": "leetcode_math::p492.py",
      "proving_time": 3.3805932127999996,
      "snark_size": 16764,
      "advice_cells": 638535,
      "fixed_cells": 1992,
      "range_check_advice_cells": 184161,
      "verify_time": 0.010451996000000002
    },
    "halo2": {
      "name": "leetcode_math::p492.rs",
      "proving_time": 3.4309108799,
      "snark_size": 16764,
      "advice_cells": 643647,
      "fixed_cells": 1026,
      "range_check_advice_cells": 184184,
      "verify_time": 0.010976240000000002
    },
    "zinnia_compile_time": 13.69333324432373
  },
  "leetcode_math::p2125": {
    "zinnia": {
      "name": "leetcode_math::p2125.py",
      "proving_time": 1.0707119346999998,
      "snark_size": 3276,
      "advice_cells": 6174,
      "fixed_cells": 2,
      "range_check_advice_cells": 0,
      "verify_time": 0.004662754
    },
    "halo2": {
      "name": "leetcode_math::p2125.rs",
      "proving_time": 1.6394267053999996,
      "snark_size": 5676,
      "advice_cells": 80039,
      "fixed_cells": 28,
      "range_check_advice_cells": 14375,
      "verify_time": 0.007298916800000001
    },
    "zinnia_compile_time": 2.587430310249329
  },
  "leetcode_matrix::p73": {
    "zinnia": {
      "name": "leetcode_matrix::p73.py",
      "proving_time": 0.7202887090000001,
      "snark_size": 3276,
      "advice_cells": 13600,
      "fixed_cells": 2,
      "range_check_advice_cells": 0,
      "verify_time": 0.0041210629
    },
    "halo2": {
      "name": "leetcode_matrix::p73.rs",
      "proving_time": 0.7481453402,
      "snark_size": 3276,
      "advice_cells": 41440,
      "fixed_cells": 2,
      "range_check_advice_cells": 0,
      "verify_time": 0.004359551800000001
    },
    "zinnia_compile_time": 3.0789755821228026
  },
  "leetcode_matrix::p2133": {
    "zinnia": {
      "name": "leetcode_matrix::p2133.py",
      "proving_time": 1.3622072739000002,
      "snark_size": 3844,
      "advice_cells": 18693,
      "fixed_cells": 26,
      "range_check_advice_cells": 4646,
      "verify_time": 0.0049497635
    },
    "halo2": {
      "name": "leetcode_matrix::p2133.rs",
      "proving_time": 1.3639324053,
      "snark_size": 3844,
      "advice_cells": 19701,
      "fixed_cells": 122,
      "range_check_advice_cells": 4600,
      "verify_time": 0.006305761300000001
    },
    "zinnia_compile_time": 0.1515206813812256
  },
  "mlalgo::mlp": {
    "zinnia": {
      "name": "mlalgo::mlp.py",
      "proving_time": 7.118886969899999,
      "snark_size": 12156,
      "advice_cells": 411094,
      "fixed_cells": 55,
      "range_check_advice_cells": 126711,
      "verify_time": 0.022146952199999997
    },
    "halo2": {
      "name": "mlalgo::mlp.rs",
      "proving_time": 5.7413360634,
      "snark_size": 24108,
      "advice_cells": 894620,
      "fixed_cells": 55,
      "range_check_advice_cells": 275663,
      "verify_time": 0.016876660199999997
    },
    "zinnia_compile_time": 13.820523667335511
  },
  "mlalgo::logistic": {
    "zinnia": {
      "name": "mlalgo::logistic.py",
      "proving_time": 72.6000633602,
      "snark_size": 319164,
      "advice_cells": 15341316,
      "fixed_cells": 525,
      "range_check_advice_cells": 3490258,
      "verify_time": 0.0866051971
    },
    "halo2": {
      "name": "mlalgo::logistic.rs",
      "proving_time": 75.773670712,
      "snark_size": 330252,
      "advice_cells": 15869545,
      "fixed_cells": 523,
      "range_check_advice_cells": 3647966,
      "verify_time": 0.0916612006
    },
    "zinnia_compile_time": 37.75442748069763
  },
  "mlalgo::decision_tree": {
    "zinnia": {
      "name": "mlalgo::decision_tree.py",
      "proving_time": 4.2394982706999995,
      "snark_size": 8256,
      "advice_cells": 289686,
      "fixed_cells": 30,
      "range_check_advice_cells": 32361,
      "verify_time": 0.0092656746
    },
    "halo2": {
      "name": "mlalgo::decision_tree.rs",
      "proving_time": 5.376154774400001,
      "snark_size": 22080,
      "advice_cells": 863259,
      "fixed_cells": 44,
      "range_check_advice_cells": 211443,
      "verify_time": 0.017470795100000002
    },
    "zinnia_compile_time": 127.02665221691132
  },
  "mlalgo::bayes": {
    "zinnia": {
      "name": "mlalgo::bayes.py",
      "proving_time": 1.435900926,
      "snark_size": 3844,
      "advice_cells": 19760,
      "fixed_cells": 48,
      "range_check_advice_cells": 5510,
      "verify_time": 0.014625187500000001
    },
    "halo2": {
      "name": "mlalgo::bayes.rs",
      "proving_time": 3.3771475400999997,
      "snark_size": 3844,
      "advice_cells": 21957,
      "fixed_cells": 50,
      "range_check_advice_cells": 6142,
      "verify_time": 0.0314529254
    },
    "zinnia_compile_time": 0.11642494201660156
  },
  "mlalgo::svm": {
    "zinnia": {
      "name": "mlalgo::svm.py",
      "proving_time": 10.6997907775,
      "snark_size": 40512,
      "advice_cells": 1655348,
      "fixed_cells": 46,
      "range_check_advice_cells": 492682,
      "verify_time": 0.024907866499999997
    },
    "halo2": {
      "name": "mlalgo::svm.rs",
      "proving_time": 10.912848774099999,
      "snark_size": 40512,
      "advice_cells": 1687473,
      "fixed_cells": 41,
      "range_check_advice_cells": 505162,
      "verify_time": 0.0211647127
    },
    "zinnia_compile_time": 42.15687894821167
  }
}